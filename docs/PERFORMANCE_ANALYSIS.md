# AnÃ¡lisis de Rendimiento y OptimizaciÃ³n - PulmoMed Backend

## ðŸ“Š EvaluaciÃ³n del Sistema (Post-OptimizaciÃ³n)

### Resumen Ejecutivo

| Aspecto | Estado Anterior | Estado Actual | Nota |
|---------|-----------------|---------------|------|
| **Escalabilidad** | âš ï¸ Media | âœ… Buena | Singleton + async permiten 20+ usuarios |
| **Calidad de CÃ³digo** | âœ… Buena | âœ… Buena | SOLID, DI, Repository Pattern |
| **Rendimiento VR** | ðŸ”´ Riesgo | âœ… Mejorado | Timeout 15s, cachÃ©, async |
| **Consumo de Memoria** | ðŸ”´ 4GB | âœ… ~500MB | Modelo embeddings ligero |
| **OptimizaciÃ³n LLM** | âš ï¸ Media | âœ… Buena | Async + cachÃ© + fallback |

---

## âœ… OPTIMIZACIONES IMPLEMENTADAS

### 1. âœ… Modelo de Embeddings Ligero

**Antes:**
```python
embedding_model: str = "BAAI/bge-m3"  # ~2-4 GB de RAM
```

**DespuÃ©s:**
```python
embedding_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
# Solo ~500MB, soporta espaÃ±ol, velocidad 3x mayor
```

**Impacto:** ReducciÃ³n de 75-85% en consumo de RAM

---

### 2. âœ… Cliente HTTP Async con Connection Pooling

**Antes:**
```python
response = httpx.post(url, timeout=60.0)  # SÃ­ncrono, bloquea 60s
```

**DespuÃ©s:**
```python
# Connection pooling compartido
_shared_client: Optional[httpx.AsyncClient] = None

async def query(self, prompt: str) -> str:
    client = self.get_http_client()  # Reutiliza conexiones
    response = await client.post(url, ...)  # No bloquea
```

**Impacto:** 20+ requests simultÃ¡neas sin bloqueo

---

### 3. âœ… Timeout Agresivo con Fallback AutomÃ¡tico

**Antes:**
```python
timeout=60.0  # 1 minuto de espera
```

**DespuÃ©s:**
```python
timeout=httpx.Timeout(15.0, connect=5.0)  # 15s mÃ¡ximo

async def query(self, prompt: str) -> str:
    try:
        response = await client.post(...)
    except httpx.TimeoutException:
        return self._mock_response(prompt)  # Fallback instantÃ¡neo
```

**Impacto:** Respuesta garantizada en <15s

---

### 4. âœ… CachÃ© de Respuestas (5 min TTL)

**Implementado:**
```python
CACHE_TTL_SECONDS = 300  # 5 minutos
MAX_CACHE_SIZE = 100

def _get_cache_key(self, state: SimulationState) -> str:
    # Cachea por: estadio + tratamiento + pack_years_bucket + resistencia
    key_data = f"{state.approx_stage}_{state.active_treatment}_{pack_years_bucket}"
    return hashlib.md5(key_data.encode()).hexdigest()[:16]

async def get_educational_feedback(self, state: SimulationState):
    cache_key = self._get_cache_key(state)
    cached = self._get_cached_response(cache_key)
    if cached is not None:
        return cached  # âš¡ Respuesta instantÃ¡nea
```

**Impacto:** Casos similares = 0ms (vs 5-10s sin cachÃ©)

---

### 5. âœ… Singleton para TeacherService

**Implementado en main.py:**
```python
_teacher_service = None

def get_teacher_service():
    global _teacher_service
    if _teacher_service is None:
        repo = get_repository()
        _teacher_service = AITeacherService(repository=repo)
    return _teacher_service
```

**Impacto:** Modelo de embeddings se carga UNA vez al startup

---

### 6. âœ… Modelo LLM mÃ¡s PequeÃ±o

**Antes:**
```python
ollama_model: str = "llama3.1:8b"  # 8B params
ollama_max_tokens: int = 1024
```

**DespuÃ©s:**
```python
ollama_model: str = "llama3.2:3b"  # 3B params, 3x mÃ¡s rÃ¡pido
ollama_max_tokens: int = 512  # Respuestas mÃ¡s cortas y rÃ¡pidas
```

**Impacto:** ReducciÃ³n ~60% en tiempo de generaciÃ³n

---

## ðŸ“ˆ MÃ‰TRICAS ESPERADAS POST-OPTIMIZACIÃ“N
            return self._mock_response(prompt)
        
        try:
            response = await self.http_client.post(
                f"{self.settings.ollama_base_url}/api/generate",
                json={
                    "model": self.settings.ollama_model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": self.settings.ollama_temperature,
                        "num_predict": 512,  # Reducir de 1024 para respuestas mÃ¡s rÃ¡pidas
                    }
                }
            )
            return response.json().get("response", "")
        except Exception as e:
            logger.error(f"LLM error: {e}")
            return self._mock_response(prompt)
```

---

### 3. Sin CachÃ© de Respuestas (Misma Pregunta = Mismo Procesamiento)

**Problema Actual:**
```python
# Cada consulta ejecuta TODO el pipeline:
# 1. Embedding generation (~500ms)
# 2. ChromaDB query (~200ms)
# 3. Cross-encoder rerank (~800ms)
# 4. LLM generation (~5-10s)
# TOTAL: 6-12 segundos CADA VEZ
```

**Impacto:**
- 20 estudiantes con casos similares = 20x el mismo trabajo
- Desperdicio de GPU/CPU
- Latencia alta constante

**SoluciÃ³n - CachÃ© LRU por Query:**
```python
# AÃ±adir en teacher_service.py
from functools import lru_cache
import hashlib

class AITeacherService:
    def __init__(self):
        # ... existing code ...
        self._response_cache = {}  # TTL cache simple
        self._cache_ttl = 300  # 5 minutos
    
    def _get_cache_key(self, state: SimulationState) -> str:
        """Genera key Ãºnica basada en estado relevante"""
        # Solo cachear por parÃ¡metros que afectan la respuesta
        relevant = f"{state.approx_stage}_{state.active_treatment}_{int(state.pack_years/10)}"
        return hashlib.md5(relevant.encode()).hexdigest()

    async def get_educational_feedback(self, state: SimulationState) -> TeacherResponse:
        cache_key = self._get_cache_key(state)
        
        # Check cache
        if cache_key in self._response_cache:
            cached, timestamp = self._response_cache[cache_key]
            if time.time() - timestamp < self._cache_ttl:
                logger.info(f"Cache hit for {cache_key}")
                return cached
        
        # ... pipeline normal ...
        response = await self._generate_response(state)
        
        # Store in cache
        self._response_cache[cache_key] = (response, time.time())
        return response
```

---

### 4. Timeout de 60 Segundos es Inaceptable para VR

**Problema Actual:**
```python
timeout=60.0  # El usuario en VR esperando 1 minuto = MAL
```

**SoluciÃ³n - Timeouts Agresivos + Fallback:**
```python
# config.py - Nuevas constantes
llm_timeout_seconds: float = 15.0  # MÃ¡ximo 15 segundos
llm_fallback_enabled: bool = True

# ollama_client.py - Timeout inteligente
async def query(self, prompt: str) -> str:
    try:
        async with asyncio.timeout(self.settings.llm_timeout_seconds):
            response = await self.http_client.post(...)
            return response.json().get("response", "")
    except asyncio.TimeoutError:
        logger.warning("LLM timeout, using fallback")
        return self._mock_response(prompt)  # Respuesta instantÃ¡nea
```

---

## âš ï¸ PROBLEMAS MODERADOS

### 5. Repository No Es Singleton Real

**Problema Actual:**
```python
# Cada request puede crear nueva instancia
service: AITeacherService = Depends(lambda: AITeacherService())
```

**Impacto:**
- Potencial de cargar el modelo de embeddings mÃºltiples veces
- Desperdicio de memoria

**SoluciÃ³n - Lifecycle Management:**
```python
# main.py
from contextlib import asynccontextmanager

# Singletons globales
_teacher_service = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Startup/shutdown lifecycle"""
    global _teacher_service
    
    # STARTUP
    logger.info("Initializing services...")
    repo = get_repository()
    repo.initialize()  # Carga embeddings UNA vez
    _teacher_service = AITeacherService(repository=repo)
    
    yield
    
    # SHUTDOWN
    logger.info("Cleaning up...")
    # Cerrar conexiones HTTP, etc.

app = FastAPI(lifespan=lifespan)

def get_teacher_service() -> AITeacherService:
    return _teacher_service

# En endpoint
@router.post("/consultar_profesor")
async def consultar_profesor(
    state: SimulationState,
    service: AITeacherService = Depends(get_teacher_service),  # âœ… Singleton
):
    ...
```

---

### 6. Cross-Encoder Reranking es Lento

**Problema Actual:**
El cÃ³digo menciona cross-encoder pero la implementaciÃ³n actual usa solo distancia de ChromaDB.

**Impacto:**
- Si se implementa cross-encoder, aÃ±ade ~500-1000ms por request
- Para VR es demasiado

**RecomendaciÃ³n:**
```python
# Mantener el rerank simple por distancia (actual)
# O usar un modelo ligero:
rerank_model: str = "cross-encoder/ms-marco-MiniLM-L-2-v2"  # MÃ¡s rÃ¡pido que L-6
```

---

### 7. Sin Rate Limiting

**Problema Actual:**
```python
# Cualquier cliente puede hacer infinitas requests
@router.post("/consultar_profesor")
async def consultar_profesor(...):
    ...
```

**Impacto:**
- Un estudiante spammeando el botÃ³n puede tumbar el servidor
- Sin protecciÃ³n contra DoS

**SoluciÃ³n - SlowAPI:**
```python
# requirements.txt
slowapi==0.1.9

# main.py
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter

# teacher_endpoint.py
@router.post("/consultar_profesor")
@limiter.limit("10/minute")  # MÃ¡ximo 10 consultas por minuto por IP
async def consultar_profesor(...):
    ...
```

---

## ðŸ’¡ OPTIMIZACIONES ADICIONALES RECOMENDADAS

### 8. Streaming para Mejor UX en VR

```python
# En lugar de esperar toda la respuesta:
# Unity muestra "..." por 10 segundos, luego texto completo

# Con streaming:
# Unity muestra texto letra por letra mientras se genera

# ollama_client.py
async def query_stream(self, prompt: str):
    """Generator que yield tokens conforme llegan"""
    async with self.http_client.stream(
        "POST",
        f"{self.settings.ollama_base_url}/api/generate",
        json={"model": self.settings.ollama_model, "prompt": prompt, "stream": True}
    ) as response:
        async for line in response.aiter_lines():
            if line:
                data = json.loads(line)
                yield data.get("response", "")
```

### 9. Modelo LLM mÃ¡s PequeÃ±o

```python
# Actual
ollama_model: str = "llama3.1:8b"  # 8B params, lento

# Recomendado para VR (3x mÃ¡s rÃ¡pido)
ollama_model: str = "llama3.2:3b"  # 3B params, suficiente para educaciÃ³n
# o
ollama_model: str = "phi3:3.8b"  # Microsoft Phi-3, muy rÃ¡pido
```

### 10. Health Check No Verifica LLM

```python
# Actual health check
return HealthCheckResponse(
    status="healthy",
    version=settings.api_version,
)

# Mejorado - verificar TODO
async def health_check():
    checks = {
        "chromadb": repo.get_collection_stats() is not None,
        "embeddings": _teacher_service.repository._embedding_model is not None,
        "ollama": OllamaClient().check_availability(),
    }
    
    all_healthy = all(checks.values())
    return {
        "status": "healthy" if all_healthy else "degraded",
        "checks": checks,
        "version": settings.api_version,
    }
```

---

## ðŸ“ˆ PLAN DE IMPLEMENTACIÃ“N PRIORITIZADO

### Fase 1: CrÃ­tico (Hacer YA)
| #  | Tarea | Impacto | Esfuerzo |
|----|-------|---------|----------|
| 1  | Hacer LLM client async | ðŸ”´ Alto | 2h |
| 2  | Reducir timeout a 15s + fallback | ðŸ”´ Alto | 30min |
| 3  | Singleton para TeacherService | ðŸŸ¡ Medio | 1h |

### Fase 2: Importante (Esta Semana)
| #  | Tarea | Impacto | Esfuerzo |
|----|-------|---------|----------|
| 4  | Implementar cachÃ© de respuestas | ðŸŸ¡ Medio | 2h |
| 5  | Cambiar a modelo embeddings ligero | ðŸŸ¡ Medio | 1h |
| 6  | Rate limiting bÃ¡sico | ðŸŸ¡ Medio | 1h |

### Fase 3: Nice to Have (PrÃ³ximas Semanas)
| #  | Tarea | Impacto | Esfuerzo |
|----|-------|---------|----------|
| 7  | Streaming de respuestas | ðŸŸ¢ Bajo | 4h |
| 8  | Modelo LLM mÃ¡s pequeÃ±o | ðŸŸ¢ Bajo | 30min |
| 9  | Health check completo | ðŸŸ¢ Bajo | 1h |

---

## ðŸŽ¯ MÃ‰TRICAS OBJETIVO

### Antes de Optimizar
| MÃ©trica | Actual |
|---------|--------|
| Latencia P50 | 6-8 segundos |
| Latencia P99 | 15-60 segundos |
| RAM del backend | 4-6 GB |
| Usuarios concurrentes | ~5-10 |

### DespuÃ©s de Optimizar
| MÃ©trica | Objetivo |
|---------|----------|
| Latencia P50 | < 3 segundos |
| Latencia P99 | < 10 segundos |
| RAM del backend | < 2 GB |
| Usuarios concurrentes | 20-30 |

---

## âœ… LO QUE ESTÃ BIEN

1. **SOLID Principles** - Bien aplicados (SRP, DIP, OCP)
2. **Repository Pattern** - Facilita cambiar a Pinecone/Weaviate
3. **Pydantic Validation** - ValidaciÃ³n robusta de inputs
4. **Logging estructurado** - Facilita debugging
5. **Mock fallback** - Sistema funciona sin Ollama
6. **ConfiguraciÃ³n centralizada** - FÃ¡cil de ajustar

---

## ðŸ› ï¸ COMANDOS PARA TESTEAR RENDIMIENTO

```bash
# Instalar herramienta de benchmarking
pip install httpx[cli]

# Test de latencia simple
time curl -X POST http://localhost:8000/api/v1/consultar_profesor \
  -H "Content-Type: application/json" \
  -d '{"age": 55, "pack_years": 30, "sensitive_tumor_volume": 5.0}'

# Test de concurrencia (10 requests simultÃ¡neas)
pip install locust
# Crear locustfile.py y correr: locust -f locustfile.py
```

---

**ConclusiÃ³n:** El cÃ³digo tiene buena arquitectura pero necesita optimizaciones de rendimiento para funcionar bien en VR con mÃºltiples usuarios. Las prioridades son: hacer async el LLM client, implementar cachÃ©, y reducir el tamaÃ±o de modelos.
